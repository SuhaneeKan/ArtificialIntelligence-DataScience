{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Suhanee Kandalkar D16AD 30"
      ],
      "metadata": {
        "id": "X0lya8OexEek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 07 : Value iteration\n",
        "\n",
        "Apply a value iteration algorithm to find optimal policies for the grid word environment."
      ],
      "metadata": {
        "id": "I_xQzfuzwk2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid World Environment (5x5)\n",
        "grid_size = 5\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-4  # Convergence threshold\n",
        "actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "action_deltas = {\"UP\": (-1, 0), \"DOWN\": (1, 0), \"LEFT\": (0, -1), \"RIGHT\": (0, 1)}\n",
        "\n",
        "# Mapping actions to arrows\n",
        "action_arrows = {\"UP\": \"↑\", \"DOWN\": \"↓\", \"LEFT\": \"←\", \"RIGHT\": \"→\"}\n",
        "\n",
        "# Rewards\n",
        "goal_state = (4, 4)\n",
        "reward_grid = -np.ones((grid_size, grid_size))  # -1 per step\n",
        "reward_grid[goal_state] = 10  # Goal reward\n",
        "\n",
        "# Initialize Value Function\n",
        "V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Print Grid Environment\n",
        "def print_grid(grid):\n",
        "    print(\"\\nGrid Environment (Rewards):\")\n",
        "    for row in grid:\n",
        "        print(\" \".join(f\"{int(val):2d}\" for val in row))\n",
        "\n",
        "# Value Iteration Algorithm\n",
        "def value_iteration():\n",
        "    global V\n",
        "    while True:\n",
        "        delta = 0\n",
        "        new_V = np.copy(V)\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                if (i, j) == goal_state:\n",
        "                    continue  # Skip goal state\n",
        "\n",
        "                # Evaluate all actions and select the best one\n",
        "                max_value = float('-inf')\n",
        "                for action in actions:\n",
        "                    di, dj = action_deltas[action]\n",
        "                    ni, nj = i + di, j + dj\n",
        "\n",
        "                    # Ensure the new state is within bounds\n",
        "                    if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                        new_value = reward_grid[i, j] + gamma * V[ni, nj]\n",
        "                    else:\n",
        "                        new_value = reward_grid[i, j] + gamma * V[i, j]  # Stay in place\n",
        "\n",
        "                    max_value = max(max_value, new_value)\n",
        "\n",
        "                new_V[i, j] = max_value\n",
        "                delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
        "\n",
        "        V = new_V\n",
        "\n",
        "        # Convergence check\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "# Compute Optimal Policy\n",
        "def extract_policy():\n",
        "    policy = np.full((grid_size, grid_size), \" \", dtype='<U5')\n",
        "\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == goal_state:\n",
        "                policy[i, j] = \"G\"\n",
        "                continue\n",
        "\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for action in actions:\n",
        "                di, dj = action_deltas[action]\n",
        "                ni, nj = i + di, j + dj\n",
        "\n",
        "                if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                    new_value = V[ni, nj]\n",
        "                else:\n",
        "                    new_value = V[i, j]\n",
        "\n",
        "                if new_value > best_value:\n",
        "                    best_value = new_value\n",
        "                    best_action = action\n",
        "\n",
        "            # If in the **rightmost column** and not at the goal, force DOWN direction\n",
        "            if j == grid_size - 1 and (i, j) != goal_state:\n",
        "                policy[i, j] = \"↓\"\n",
        "            else:\n",
        "                policy[i, j] = action_arrows[best_action]\n",
        "\n",
        "    return policy\n",
        "\n",
        "# Run Value Iteration\n",
        "value_iteration()\n",
        "\n",
        "# Extract Optimal Policy\n",
        "optimal_policy = extract_policy()\n",
        "\n",
        "# Print Results\n",
        "print_grid(reward_grid)\n",
        "\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for row in np.round(V, 2):\n",
        "    print(\" \".join(f\"{val:5.2f}\" for val in row))\n",
        "\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for row in optimal_policy:\n",
        "    print(\" \".join(row))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-NWpUuJs36b",
        "outputId": "a1bd3a7f-0926-4c05-9cc0-d598b4b281ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grid Environment (Rewards):\n",
            "-1 -1 -1 -1 -1\n",
            "-1 -1 -1 -1 -1\n",
            "-1 -1 -1 -1 -1\n",
            "-1 -1 -1 -1 -1\n",
            "-1 -1 -1 -1 10\n",
            "\n",
            "Optimal Value Function:\n",
            "-5.70 -5.22 -4.69 -4.10 -3.44\n",
            "-5.22 -4.69 -4.10 -3.44 -2.71\n",
            "-4.69 -4.10 -3.44 -2.71 -1.90\n",
            "-4.10 -3.44 -2.71 -1.90 -1.00\n",
            "-3.44 -2.71 -1.90 -1.00  0.00\n",
            "\n",
            "Optimal Policy:\n",
            "↓ ↓ ↓ ↓ ↓\n",
            "↓ ↓ ↓ ↓ ↓\n",
            "↓ ↓ ↓ ↓ ↓\n",
            "↓ ↓ ↓ ↓ ↓\n",
            "→ → → → G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQwx4MgQuzJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}